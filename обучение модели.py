import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
import os
import json
import pickle


class RoleDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class RoleClassifier(nn.Module):
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """

    def __init__(self, model_name='cointegrated/rubert-tiny', num_labels=8):
        super().__init__()
        self.model_name = model_name
        self.num_labels = num_labels
        self.transformer = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )

    def forward(self, input_ids, attention_mask, labels=None):
        return self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )


def prepare_data(csv_path='Barotrauma_dataset_full.csv'):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv(csv_path, encoding='utf-8')

    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print("–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö:")
    print(df.head())

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    if 'text' not in df.columns or 'specialization' not in df.columns:
        raise ValueError("–í –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'text' –∏ 'specialization'")

    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ (—Ä–æ–ª–∏)
    label_encoder = LabelEncoder()
    labels_encoded = label_encoder.fit_transform(df['specialization'])

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫
    label_map = {i: label for i, label in enumerate(label_encoder.classes_)}

    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(label_map)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–æ–ª–µ–π:")
    for idx, role in label_map.items():
        print(f"  {idx}: {role}")

    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    X_train, X_val, y_train, y_val = train_test_split(
        df['text'].values,
        labels_encoded,
        test_size=0.2,
        random_state=42,
        stratify=labels_encoded
    )

    print(f"\n–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train)}")
    print(f"–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_val)}")

    return X_train, X_val, y_train, y_val, label_map, label_encoder


def train_model(X_train, y_train, X_val, y_val, label_map, model_name='cointegrated/rubert-tiny'):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    """
    print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {model_name}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(label_map)}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # –°–æ–∑–¥–∞–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å
    model = RoleClassifier(model_name=model_name, num_labels=len(label_map))

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = RoleDataset(X_train, y_train, tokenizer)
    val_dataset = RoleDataset(X_val, y_val, tokenizer)

    # –°–æ–∑–¥–∞–µ–º DataLoader
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    model.to(device)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º torch.optim.AdamW
    optimizer = optim.AdamW(model.parameters(), lr=2e-5)
    num_epochs = 5

    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    train_losses = []
    val_losses = []
    accuracies = []

    # –û–±—É—á–µ–Ω–∏–µ
    for epoch in range(num_epochs):
        print(f"\n{'=' * 50}")
        print(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}")
        print('=' * 50)

        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        train_loss = 0
        train_batches = 0

        for batch in tqdm(train_loader, desc="–û–±—É—á–µ–Ω–∏–µ", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

            loss = outputs.loss
            train_loss += loss.item()
            train_batches += 1

            loss.backward()
            optimizer.step()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è", leave=False):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                val_loss += outputs.loss.item()

                # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
                predictions = torch.argmax(outputs.logits, dim=1)
                correct += (predictions == labels).sum().item()
                total += labels.size(0)

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        accuracy = correct / total

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        accuracies.append(accuracy)

        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏ {epoch + 1}:")
        print(f"  –°—Ä–µ–¥–Ω—è—è loss –æ–±—É—á–µ–Ω–∏—è: {avg_train_loss:.4f}")
        print(f"  –°—Ä–µ–¥–Ω—è—è loss –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {avg_val_loss:.4f}")
        print(f"  –¢–æ—á–Ω–æ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {accuracy:.4f}")

    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n{'=' * 50}")
    print("–ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø")
    print('=' * 50)
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracies[-1]:.4f}")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(accuracies):.4f} (—ç–ø–æ—Ö–∞ {accuracies.index(max(accuracies)) + 1})")

    return model, tokenizer


def save_model_pth(model, tokenizer, label_map, label_encoder, output_dir='role_model_pth'):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .pth
    """
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(output_dir, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ .pth
    model_path = os.path.join(output_dir, 'role_classifier.pth')
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': {
            'model_name': model.model_name,
            'num_labels': model.num_labels
        },
        'label_map': label_map,
        'class_names': label_encoder.classes_.tolist()
    }, model_path)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç–¥–µ–ª—å–Ω–æ (—á–µ—Ä–µ–∑ pickle)
    tokenizer_path = os.path.join(output_dir, 'tokenizer.pkl')
    with open(tokenizer_path, 'wb') as f:
        pickle.dump(tokenizer, f)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫ –≤ JSON –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    with open(os.path.join(output_dir, 'label_map.json'), 'w', encoding='utf-8') as f:
        json.dump(label_map, f, ensure_ascii=False, indent=2)

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ñ–∞–π–ª —Å –∫–æ–¥–æ–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (–≤—Å—ë –≤ –æ–¥–Ω–æ–º)
    usage_code = '''# role_model_pth/usage_example.py
# –ö–æ–¥ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

import torch
import pickle
import json
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def load_role_model(model_dir='role_model_pth'):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–æ–ª–µ–π –∏–∑ .pth —Ñ–∞–π–ª–∞

    Args:
        model_dir: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –º–æ–¥–µ–ª—å—é

    Returns:
        —Å–ª–æ–≤–∞—Ä—å —Å –º–æ–¥–µ–ª—å—é, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –∏ label_map
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    checkpoint = torch.load(f'{model_dir}/role_classifier.pth', map_location='cpu')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    with open(f'{model_dir}/tokenizer.pkl', 'rb') as f:
        tokenizer = pickle.load(f)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    model_config = checkpoint['model_config']

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ç–æ–π –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    model = AutoModelForSequenceClassification.from_pretrained(
        model_config['model_name'],
        num_labels=model_config['num_labels']
    )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to('cpu')
    model.eval()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫
    label_map = checkpoint['label_map']

    return {
        'model': model,
        'tokenizer': tokenizer,
        'label_map': label_map
    }

def predict_role(model_dict, text):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–æ–ª–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞

    Args:
        model_dict: —Å–ª–æ–≤–∞—Ä—å —Å –º–æ–¥–µ–ª—å—é, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –∏ label_map
        text: —Ç–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

    Returns:
        tuple: (—Ä–æ–ª—å, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
    """
    model = model_dict['model']
    tokenizer = model_dict['tokenizer']
    label_map = model_dict['label_map']

    with torch.no_grad():
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        outputs = model(**inputs)
        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_id = torch.argmax(probabilities, dim=-1).item()
        confidence = probabilities[0][predicted_id].item()

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–æ–ª–∏
        predicted_role = label_map.get(str(predicted_id), f"–ö–ª–∞—Å—Å {predicted_id}")

        return predicted_role, confidence

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model_dict = load_role_model()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_texts = [
        "–°—Ä–æ—á–Ω–æ, –∫–∞–ø–∏—Ç–∞–Ω, –º–µ—Ö–∞–Ω–∏–∫–∞ –±–∞—Ä–∞—Ö–ª–∏—Ç!",
        "–î–æ–∫—Ç–æ—Ä, –Ω—É–∂–Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –ø–æ–º–æ—â—å!",
        "–ò–Ω–∂–µ–Ω–µ—Ä, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∏—Å—Ç–µ–º—É"
    ]

    print("\\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
    print("=" * 50)

    for text in test_texts:
        role, confidence = predict_role(model_dict, text)
        print(f"–¢–µ–∫—Å—Ç: {text}")
        print(f"–†–æ–ª—å: {role} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%})")
        print()
'''

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø–∞–ø–∫—É –º–æ–¥–µ–ª–∏
    usage_path = os.path.join(output_dir, 'usage_example.py')
    with open(usage_path, 'w', encoding='utf-8') as f:
        f.write(usage_code)

    # –°–æ–∑–¥–∞–µ–º –µ—â–µ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è –≤–∞—à–µ–≥–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–¥–∞
    simple_loader_code = '''# role_model_pth/simple_loader.py
# –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–∞—à–µ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –º–æ–¥—É–ª—è

import torch
import pickle

def load_role_classifier_simple(model_dir='role_model_pth'):
    """
    –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    checkpoint = torch.load(f'{model_dir}/role_classifier.pth', map_location='cpu')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    with open(f'{model_dir}/tokenizer.pkl', 'rb') as f:
        tokenizer = pickle.load(f)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    model_config = checkpoint['model_config']

    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–¥–µ
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = AutoModelForSequenceClassification.from_pretrained(
        model_config['model_name'],
        num_labels=model_config['num_labels']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to('cpu')
    model.eval()

    # –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫
    label_map = checkpoint['label_map']

    # –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    def predict(text):
        with torch.no_grad():
            inputs = tokenizer(
                text,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=128
            )

            outputs = model(**inputs)
            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_id].item()

            predicted_role = label_map.get(str(predicted_id), f"–ö–ª–∞—Å—Å {predicted_id}")

            return predicted_role, confidence

    return predict

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –≤–∞—à–µ–º –∫–æ–¥–µ:
# from simple_loader import load_role_classifier_simple
# predict = load_role_classifier_simple()
# role, confidence = predict("–ö–∞–ø–∏—Ç–∞–Ω, –ø–æ–º–æ–≥–∏—Ç–µ!")
'''

    simple_loader_path = os.path.join(output_dir, 'simple_loader.py')
    with open(simple_loader_path, 'w', encoding='utf-8') as f:
        f.write(simple_loader_code)

    print(f"\n–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É: {output_dir}")
    print("\n–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print(f"  ‚úì role_classifier.pth - –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å (–≤–µ—Å–∞ + –∫–æ–Ω—Ñ–∏–≥)")
    print(f"  ‚úì tokenizer.pkl - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä")
    print(f"  ‚úì label_map.json - –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫")
    print(f"  ‚úì usage_example.py - –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
    print(f"  ‚úì simple_loader.py - –ø—Ä–æ—Å—Ç–æ–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è –≤–∞—à–µ–≥–æ –∫–æ–¥–∞")

    print("\n–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –≤–∞—à–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–º –º–æ–¥—É–ª–µ:")
    print("=" * 60)
    print("""
# 1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –ø–∞–ø–∫—É role_model_pth —Ä—è–¥–æ–º —Å –≤–∞—à–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º
# 2. –í –Ω–∞—á–∞–ª–µ –≤–∞—à–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ –¥–æ–±–∞–≤—å—Ç–µ:

def load_role_model():
    import torch
    import pickle
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    checkpoint = torch.load('role_model_pth/role_classifier.pth', map_location='cpu')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    with open('role_model_pth/tokenizer.pkl', 'rb') as f:
        tokenizer = pickle.load(f)

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model_config = checkpoint['model_config']
    model = AutoModelForSequenceClassification.from_pretrained(
        model_config['model_name'],
        num_labels=model_config['num_labels']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to('cpu')
    model.eval()

    label_map = checkpoint['label_map']

    return model, tokenizer, label_map

# 3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ –≤–∞—à–µ–º –∫–æ–¥–µ:

model, tokenizer, label_map = load_role_model()

def predict_role(text):
    with torch.no_grad():
        inputs = tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )

        outputs = model(**inputs)
        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_id = torch.argmax(probabilities, dim=-1).item()
        confidence = probabilities[0][predicted_id].item()

        predicted_role = label_map.get(str(predicted_id), f"–ö–ª–∞—Å—Å {predicted_id}")

        return predicted_role, confidence

# 4. –í –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ:
# role, confidence = predict_role(—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç)
""")
    print("=" * 60)


def test_model_with_examples(model, tokenizer, label_map):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    """
    print("\n" + "=" * 50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê –ü–†–ò–ú–ï–†–ê–•")
    print("=" * 50)

    test_examples = [
        "–°—Ä–æ—á–Ω–æ, –∫–∞–ø–∏—Ç–∞–Ω, –º–µ—Ö–∞–Ω–∏–∫–∞ –±–∞—Ä–∞—Ö–ª–∏—Ç, –¥–µ–π—Å—Ç–≤—É–π!",
        "–ö–∞–ø–∏—Ç–∞–Ω, –≤—Å—ë –≥—Ä–µ–º–∏—Ç, –ø–æ—á–∏–Ω–∏ –º–µ—Ö–∞–Ω–∏–∫—É!",
        "–ú–µ—Ö–∞–Ω–∏–∫, –ø—Ä–æ–≤–µ—Ä—å –¥–≤–∏–≥–∞—Ç–µ–ª—å!",
        "–î–æ–∫—Ç–æ—Ä, –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å!",
        "–ò–Ω–∂–µ–Ω–µ—Ä, —Å–∏—Å—Ç–µ–º–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∞!"
    ]

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()

    for example in test_examples:
        with torch.no_grad():
            inputs = tokenizer(
                example,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=128
            ).to(device)

            outputs = model(**inputs)
            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_id].item()

            predicted_role = label_map.get(str(predicted_id), f"–ö–ª–∞—Å—Å {predicted_id}")

            print(f"\n–ü—Ä–∏–º–µ—Ä: {example}")
            print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ä–æ–ª—å: {predicted_role}")
            print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            top_k = 3
            probs, indices = torch.topk(probabilities[0], top_k)
            print(f"–¢–æ–ø-{top_k} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
            for i in range(top_k):
                role = label_map.get(str(indices[i].item()), f"–ö–ª–∞—Å—Å {indices[i].item()}")
                prob = probs[i].item()
                print(f"  {i + 1}. {role}: {prob:.2%}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    """
    print("=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –†–û–õ–ï–ô –î–õ–Ø BAROTRAUMA")
    print("–§–æ—Ä–º–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: .pth")
    print("=" * 70)

    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\n[1/4] –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•...")
        X_train, X_val, y_train, y_val, label_map, label_encoder = prepare_data()

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\n[2/4] –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò...")
        model, tokenizer = train_model(X_train, y_train, X_val, y_val, label_map)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\n[3/4] –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò...")
        test_model_with_examples(model.transformer, tokenizer, label_map)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ .pth —Ñ–æ—Ä–º–∞—Ç–µ
        print("\n[4/4] –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò...")
        save_model_pth(model, tokenizer, label_map, label_encoder)

        print("\n" + "=" * 70)
        print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("=" * 70)

        print("\nüìÅ –ü–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é: role_model_pth")
        print("üìÑ –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª: role_classifier.pth")
        print("\n–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –≤–∞—à–µ–º –∫–æ–¥–µ –ø—Ä–æ—Å—Ç–æ:")
        print("1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –ø–∞–ø–∫—É 'role_model_pth' —Ä—è–¥–æ–º —Å –≤–∞—à–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º")
        print("2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–¥ –∏–∑ simple_loader.py –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ —Å–≤–æ–π –∑–∞–≥—Ä—É–∑—á–∏–∫")

    except FileNotFoundError as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª 'Barotrauma_dataset_full.csv' –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()