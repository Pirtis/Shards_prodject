{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "# # Сохранение в CSV файл\n",
    "# combined_df.to_csv('Barotrauma_dataset_full.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Загрузка объединенного датасета\n",
    "df = pd.read_csv('Barotrauma_dataset_full.csv')\n",
    "\n",
    "# Определяем mapping для группировки специализаций\n",
    "SPECIALIZATION_MAPPING = {\n",
    "    # Медик/Врач/Док группируем в Медик\n",
    "    'медик': 'Медик', 'врач': 'Медик', 'док': 'Медик', 'доктор': 'Медик',\n",
    "    'medic': 'Медик', 'doctor': 'Медик',\n",
    "    \n",
    "    # Капитан\n",
    "    'капитан': 'Капитан', 'captain': 'Капитан',\n",
    "    \n",
    "    # Охранник\n",
    "    'охранник': 'Охранник', 'security': 'Охранник', 'guard': 'Охранник',\n",
    "    \n",
    "    # Инженер\n",
    "    'инженер': 'Инженер', 'engineer': 'Инженер',\n",
    "    \n",
    "    # Механик\n",
    "    'механик': 'Механик', 'mechanic': 'Механик',\n",
    "    \n",
    "    # Помощник\n",
    "    'помощник': 'Помощник', 'assistant': 'Помощник',\n",
    "    \n",
    "    # Ближайший бот\n",
    "    'near': 'Near', 'ближайший': 'Near', 'бот': 'Near', 'bot': 'Near',\n",
    "    \n",
    "    # Общее обращение\n",
    "    'all': 'All', 'все': 'All'\n",
    "}\n",
    "\n",
    "# Функция для нормализации специализации\n",
    "def normalize_specialization(spec):\n",
    "    spec_lower = str(spec).lower().strip()\n",
    "    \n",
    "    # Прямое соответствие\n",
    "    if spec_lower in SPECIALIZATION_MAPPING:\n",
    "        return SPECIALIZATION_MAPPING[spec_lower]\n",
    "    \n",
    "    # Поиск вхождений в тексте\n",
    "    for key, value in SPECIALIZATION_MAPPING.items():\n",
    "        if key in spec_lower:\n",
    "            return value\n",
    "    \n",
    "    # Если не нашли - возвращаем оригинал (или можно задать значение по умолчанию)\n",
    "    return 'All'\n",
    "\n",
    "# Применяем нормализацию\n",
    "df['normalized_specialization'] = df['specialization'].apply(normalize_specialization)\n",
    "\n",
    "# Проверяем распределение\n",
    "print(\"Распределение по классам:\")\n",
    "print(df['normalized_specialization'].value_counts())\n",
    "\n",
    "def remove_addressing(text, specialization):\n",
    "    # Создаем список всех синонимов для данной специализации\n",
    "    synonyms = []\n",
    "    for key, value in SPECIALIZATION_MAPPING.items():\n",
    "        if value == specialization:\n",
    "            synonyms.append(key)\n",
    "    \n",
    "    # Добавляем русские и английские варианты\n",
    "    additional_synonyms = {\n",
    "        'Медик': ['медик', 'врач', 'док', 'доктор', 'medic', 'doctor'],\n",
    "        'Капитан': ['капитан', 'captain'],\n",
    "        'Охранник': ['охранник', 'security', 'guard'],\n",
    "        'Инженер': ['инженер', 'engineer'],\n",
    "        'Механик': ['механик', 'mechanic'],\n",
    "        'Помощник': ['помощник', 'assistant'],\n",
    "        'Near': ['near', 'ближайший', 'бот', 'bot'],\n",
    "        'All': ['all', 'все']\n",
    "    }\n",
    "    \n",
    "    if specialization in additional_synonyms:\n",
    "        synonyms.extend(additional_synonyms[specialization])\n",
    "    \n",
    "    # Удаляем дубликаты\n",
    "    synonyms = list(set(synonyms))\n",
    "    \n",
    "    cleaned_text = str(text)\n",
    "    \n",
    "    # Удаляем каждое возможное обращение\n",
    "    for synonym in synonyms:\n",
    "        # Паттерны для удаления (с учетом разных форм)\n",
    "        patterns = [\n",
    "            rf'\\b{re.escape(synonym)}\\b[\\s,]*',\n",
    "            rf'\\b{re.escape(synonym.capitalize())}\\b[\\s,]*',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Удаляем лишние пробелы и запятые\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r'^\\s*,\\s*', '', cleaned_text)  # Удаляем запятую в начале\n",
    "    cleaned_text = re.sub(r'\\s*,\\s*$', '', cleaned_text)  # Удаляем запятую в конце\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Применяем функцию ко всему датасету\n",
    "df['cleaned_text'] = df.apply(\n",
    "    lambda row: remove_addressing(row['text'], row['normalized_specialization']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Фиксируем 8 классов\n",
    "FINAL_CLASSES = ['Капитан', 'Охранник', 'Медик', 'Инженер', 'Механик', 'Помощник', 'Near', 'All']\n",
    "\n",
    "# Кодируем специализации\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(FINAL_CLASSES)  # Фиксируем классы\n",
    "df['specialization_encoded'] = label_encoder.transform(df['normalized_specialization'])\n",
    "\n",
    "# Параметры для текста\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Создаем улучшенный токенизатор для PyTorch\n",
    "class TextTokenizer:\n",
    "    def __init__(self, max_vocab_size, oov_token=\"<OOV>\"):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        word_counts = {}\n",
    "        for text in texts:\n",
    "            # Более качественная токенизация с учетом пунктуации\n",
    "            words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "            for word in words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Сортируем слова по частоте\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Создаем словарь\n",
    "        self.word_index = {self.oov_token: 0}\n",
    "        self.index_word = {0: self.oov_token}\n",
    "        \n",
    "        idx = 1\n",
    "        for word, count in sorted_words:\n",
    "            if idx < self.max_vocab_size:\n",
    "                self.word_index[word] = idx\n",
    "                self.index_word[idx] = word\n",
    "                idx += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        self.vocab_size = len(self.word_index)\n",
    "        print(f\"Создан словарь размером: {self.vocab_size} слов\")\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "            sequence = []\n",
    "            for word in words:\n",
    "                sequence.append(self.word_index.get(word, 0))  # 0 для OOV\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "# Инициализируем и обучаем токенизатор\n",
    "tokenizer = TextTokenizer(MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "# Преобразуем текст в последовательности\n",
    "X_sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "\n",
    "# Функция для padding последовательностей\n",
    "def pad_sequences(sequences, maxlen, padding='post', truncating='post', value=0):\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            pad_length = maxlen - len(seq)\n",
    "            if padding == 'post':\n",
    "                seq = seq + [value] * pad_length\n",
    "            else:\n",
    "                seq = [value] * pad_length + seq\n",
    "        result.append(seq)\n",
    "    return np.array(result)\n",
    "\n",
    "X_padded = pad_sequences(X_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Подготовка меток\n",
    "y = df['specialization_encoded'].values\n",
    "\n",
    "# Проверяем баланс классов\n",
    "print(\"\\nРаспределение классов:\")\n",
    "for i, class_name in enumerate(FINAL_CLASSES):\n",
    "    count = np.sum(y == i)\n",
    "    print(f\"{class_name}: {count} samples ({count/len(y)*100:.2f}%)\")\n",
    "\n",
    "# Разделение на train/test\n",
    "X_train, X_test, y_train, y_test, text_train, text_test = train_test_split(\n",
    "    X_padded, y, df['text'], test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Размер тренировочной выборки: {len(X_train)}\")\n",
    "print(f\"Размер тестовой выборки: {len(X_test)}\")\n",
    "print(f\"Количество классов: {len(FINAL_CLASSES)}\")\n",
    "print(f\"Размер словаря: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Создаем улучшенную PyTorch модель\n",
    "class SpecializationClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=2, dropout=0.3):\n",
    "        super(SpecializationClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 из-за bidirectional\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Используем последний hidden state из bidirectional LSTM\n",
    "        hidden_forward = hidden[-2, :, :]  # forward direction\n",
    "        hidden_backward = hidden[-1, :, :]  # backward direction\n",
    "        out = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Параметры модели\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "NUM_CLASSES = len(FINAL_CLASSES)\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Создаем модель\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используется устройство: {device}\")\n",
    "\n",
    "model = SpecializationClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=NUM_CLASSES,\n",
    "    n_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(f\"Модель создана: {sum(p.numel() for p in model.parameters()):,} параметров\")\n",
    "\n",
    "# Функция для создания DataLoader\n",
    "def create_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    dataset = TensorDataset(\n",
    "        torch.LongTensor(X),\n",
    "        torch.LongTensor(y)\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Создаем DataLoader'ы\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=32)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# Функция для вычисления accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = labels.size(0)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Функция для обучения\n",
    "def train_model(model, train_loader, test_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        batches = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping для стабильности\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += calculate_accuracy(outputs, batch_y)\n",
    "            batches += 1\n",
    "        \n",
    "        train_accuracy = total_accuracy / batches\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Валидация\n",
    "        val_accuracy = evaluate_model(model, test_loader)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], LR: {current_lr:.6f}, Loss: {avg_loss:.4f}, '\n",
    "              f'Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Сохраняем лучшую модель\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'val_accuracies': val_accuracies\n",
    "            }, 'best_specialization_model_8classes.pth')\n",
    "            print(f'Новая лучшая модель сохранена с точностью: {best_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            accuracy = calculate_accuracy(outputs, batch_y)\n",
    "            total_accuracy += accuracy\n",
    "            batches += 1\n",
    "    \n",
    "    return total_accuracy / batches\n",
    "\n",
    "# Обучаем модель\n",
    "print(\"\\nНачало обучения...\")\n",
    "train_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model, train_loader, test_loader, epochs=30, lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Функция для сохранения всех необходимых файлов\n",
    "def save_model_artifacts(model, tokenizer, label_encoder, special_mapping, final_classes, max_sequence_length):\n",
    "    \"\"\"Сохраняет все необходимые артефакты для использования модели\"\"\"\n",
    "    \n",
    "    # Создаем папку для модели\n",
    "    model_dir = \"specialization_model_package\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Сохраняем веса модели\n",
    "    torch.save(model.state_dict(), f\"{model_dir}/model_weights.pth\")\n",
    "    \n",
    "    # 2. Сохраняем архитектуру модели и параметры\n",
    "    model_config = {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'output_dim': len(final_classes),\n",
    "        'n_layers': NUM_LAYERS,\n",
    "        'dropout': DROPOUT,\n",
    "        'max_sequence_length': max_sequence_length,\n",
    "        'bidirectional': True\n",
    "    }\n",
    "    \n",
    "    with open(f\"{model_dir}/model_config.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 3. Сохраняем токенизатор\n",
    "    with open(f\"{model_dir}/tokenizer.pkl\", 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    \n",
    "    # 4. Сохраняем label encoder\n",
    "    with open(f\"{model_dir}/label_encoder.pkl\", 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    # 5. Сохраняем mapping специализаций\n",
    "    with open(f\"{model_dir}/specialization_mapping.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(special_mapping, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 6. Сохраняем финальные классы\n",
    "    with open(f\"{model_dir}/final_classes.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_classes, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Все файлы модели сохранены в папку: {model_dir}\")\n",
    "\n",
    "# Сохраняем все артефакты модели\n",
    "save_model_artifacts(model, tokenizer, label_encoder, SPECIALIZATION_MAPPING, FINAL_CLASSES, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Загружаем лучшую модель для предсказаний\n",
    "checkpoint = torch.load('best_specialization_model_8classes.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nЛучшая точность на валидации: {checkpoint['best_accuracy']:.2f}%\")\n",
    "\n",
    "def predict_and_clean_text(model, tokenizer, label_encoder, input_texts):\n",
    "    \"\"\"\n",
    "    Предсказывает специализацию и очищает текст от обращений\n",
    "    \"\"\"\n",
    "    # Преобразуем текст в последовательности\n",
    "    sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Предсказываем\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.LongTensor(padded_sequences).to(device)\n",
    "        outputs = model(inputs)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    predicted_specializations = label_encoder.inverse_transform(predicted_classes)\n",
    "    \n",
    "    # Очищаем текст от обращений\n",
    "    cleaned_texts = []\n",
    "    for text, spec in zip(input_texts, predicted_specializations):\n",
    "        cleaned_text = remove_addressing(text, spec)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    return predicted_specializations, cleaned_texts\n",
    "\n",
    "# Предсказываем для всего датасета\n",
    "all_texts = df['text'].tolist()\n",
    "predicted_specs, cleaned_texts = predict_and_clean_text(model, tokenizer, label_encoder, all_texts)\n",
    "\n",
    "# Вычисляем accuracy на всем датасете\n",
    "true_labels = df['normalized_specialization'].values\n",
    "accuracy = np.mean(predicted_specs == true_labels) * 100\n",
    "print(f\"\\nТочность на всем датасете: {accuracy:.2f}%\")\n",
    "\n",
    "# Создаем финальный датасет\n",
    "result_df = pd.DataFrame({\n",
    "    'original_text': df['text'],\n",
    "    'true_specialization': df['normalized_specialization'],\n",
    "    'predicted_specialization': predicted_specs,\n",
    "    'cleaned_text': cleaned_texts,\n",
    "    'is_correct': (predicted_specs == true_labels)\n",
    "})\n",
    "\n",
    "# Сохраняем результат\n",
    "result_df.to_csv('predicted_specializations_8classes_pytorch.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Файл 'predicted_specializations_8classes_pytorch.csv' успешно создан!\")\n",
    "print(f\"Всего обработано: {len(result_df)} текстов\")\n",
    "print(f\"Используемые классы: {FINAL_CLASSES}\")\n",
    "\n",
    "# Тестируем на различных обращениях\n",
    "test_texts = [\n",
    "    \"Доктор, проверь пациента\",\n",
    "    \"Врач, подойди сюда\", \n",
    "    \"Медик, нужна помощь\",\n",
    "    \"Механик, почини двигатель\",\n",
    "    \"Инженер, проверь системы\",\n",
    "    \"Капитан, доложите ситуацию\",\n",
    "    \"Ближайший бот, иди сюда\",\n",
    "    \"Все, внимание!\",\n",
    "    \"Near, come here\",\n",
    "    \"All hands on deck!\"\n",
    "]\n",
    "\n",
    "test_specs, test_cleaned = predict_and_clean_text(model, tokenizer, label_encoder, test_texts)\n",
    "\n",
    "test_results = pd.DataFrame({\n",
    "    'input_text': test_texts,\n",
    "    'predicted_specialization': test_specs,\n",
    "    'cleaned_text': test_cleaned\n",
    "})\n",
    "\n",
    "print(\"\\nТестовые примеры:\")\n",
    "print(test_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Ключевые улучшения:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Bidirectional LSTM для лучшего понимания контекста\")\n",
    "print(\"2. Batch Normalization для стабильности обучения\")\n",
    "print(\"3. Gradient Clipping для предотвращения взрыва градиентов\")\n",
    "print(\"4. Learning Rate Scheduling\")\n",
    "print(\"5. Weight Decay для регуляризации\")\n",
    "print(\"6. Улучшенная токенизация с учетом пунктуации\")\n",
    "print(\"7. Стратифицированное разделение данных\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
